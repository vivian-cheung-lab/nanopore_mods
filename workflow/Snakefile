# Snakemake for nanopore data analysis

import glob
import gzip
import pdb
import pickle
import subprocess
import sys

import pandas
import numpy as np
import pysam

# add local modules to this
sys.path.append('py/')

# import seq.nanopore.align
import seq.nanopore.polish
import seq.nanopore.squiggle.read_event
# import seq.nanopore.squiggle.matrix
import seq.nanopore.squiggle.stat.event_stats

DATA_DIR = '../data/AANCR_IVT_1/mop_preprocess/output/'

# fast5_dir = DATA_DIR + '/Fastq/nanopore/IVT_AANCR/'

# these are just chr19, with the sequence of AANCR
# AANCR_fasta = '/home/jtburd/work/seq/nanopore/RNA/polish/f5c/IVT/RDD/chr19_AANCR_clone_only_230411.fa'
AANCR_fasta = '../data/AANCR_IVT_1/AANCR_IVT.fa'
# minimap2_index = 'index/chr19_AANCR_clone_only_230411.mmi'

regions_file = '../data/AANCR_IVT_1/regions.bed'

# coordinates of AANCR
region = 'AANCR_IVT:1-1908'

sample_table = pandas.read_csv('IVT_samples.csv')

sample_base = sample_table.FASTQ_file_base_name.tolist()
sample_base = sample_base
modified_sample_table = sample_table[ sample_table.mod_concentration != 'none' ]
modified_sample_table.set_index('FASTQ_file_base_name', inplace=True)
modified_sample_base = modified_sample_table.index.tolist()
control_sample_table = sample_table[sample_table.mod_concentration=='none']
control_sample_table.set_index('short_name', inplace=True)

# this specifies what things we're trying to compute
rule all:
	input:
		# this is only reads which passed Guppy's quality filter
		['bam_1/' + s + '.bam'
			for s in sample_base],
                ['RDD_3/' + s + '.csv.gz'
			for s in sample_base],
                ['events/' + s + '.mergedEvent.tsv.gz'
			for s in sample_base],
                ['event_stats/' + region + '/' + s + '.csv.gz'
			for s in sample_base],
		'scripts/error_rates_2.xlsx',
		'output/Dwell time for sugar modifications, 100%.pdf'

# Writes out a combined .fastq.gz file.
# Deprecated (MOP2 is now doing this).
rule fastq_gz:
	input: 
		'/net/schuylkill/data/Fastq/nanopore/IVT_AANCR/{sample_name}/'
	output:
		'fastq/{sample_name}.fastq.gz'
	run:
		# get location of guppy_output/ directory
		guppy_output_dirs = (glob.glob(f'{input}/*/guppy_output/')
			+ glob.glob(f'{input}/*/*/guppy_output/'))
		if len(guppy_output_dirs) == 1:
			guppy_output_dir = guppy_output_dirs[0]
		subprocess.run(f'cat {guppy_output_dir}/pass/*.fastq.gz > fastq/{wildcards.sample_name}.fastq.gz',
			shell=True)
		# XXX now including files from any subdirectory called "guppy_output"
		# 'cat {input}/**/guppy_output/pass/*.fastq.gz > fastq/{wildcards.sample_name}.fastq.gz'

# Constructs a minimap2 index of AANCR.
#rule minimap2_index:
#	input:
#		# chr19 sequence, with just the modified version of AANCR
#		# (masked with N's elsewhere)
#		AANCR_fasta
#	output:
#		minimap2_index
#	shell:
#		'minimap2 -d {output} {input}'

# Aligns reads using minimap2.
# rule minimap2:
#	input: 
#		'fastq/{sample_name}.fastq.gz',
#		minimap2_index
#	output:
#		'bam_no_splicing/{sample_name}.bam'
#	shell:
#		# XXX avoid using --write-index for now
#		'minimap2 -t 32 -a -L -x map-ont {minimap2_index} fastq/{wildcards.sample_name}.fastq.gz | samtools sort - -o bam_no_splicing/{wildcards.sample_name}.bam; samtools index bam_no_splicing/{wildcards.sample_name}.bam'

# Gets statistics about reads.
rule sample_stats_1:
	input:
		bam='bam_no_splicing/{sample_name}.bam'
	output:
		stats_file='summary/sample_stats/{sample_name}.stats_and_flow_cell.csv'
	run:
		input_dir = DATA_DIR + '/Fastq/nanopore/IVT_AANCR/' + wildcards.sample_name
		guppy_output_dirs = (glob.glob(input_dir + '/*/guppy_output/')
			+ glob.glob(input_dir + '/*/*/guppy_output/'))
		if len(guppy_output_dirs) == 1:
			guppy_output_dir = guppy_output_dirs[0]
		bam_file = 'bam_no_splicing/' + wildcards.sample_name
		seq.nanopore.align.writeStatistics(guppy_output_dir, bam_file,
			output.stats_file)

rule sample_stats:
	input:
		['summary/sample_stats/' + s + '.stats_and_flow_cell.csv'
			for s in sample_base]
	output:
		'summary/sample_stats.csv'
	run:
		# parse stats summaries using Pandas
		stats_files = [pandas.read_csv(f) for f in input]
		# combine them
		stats = pandas.concat(stats_files)
		# join with sample info
		sample_info = pandas.read_csv('IVT_samples.csv')
		sample_info.columns = ['Sample', 'Short name', 'Mod. concentration']
		stats = sample_info.merge(stats)
		# write them out
		stats.to_csv(output[0], index=False)


# Writes out the reads which passed Guppy's quality control.
rule bam_pass:
	input:
		input_bam=DATA_DIR+'alignment/{sample_name}_s.bam',
		qc_file=DATA_DIR+'QC_files/{sample_name}_final_summary.stats'
	output:
		output_bam='bam_1/{sample_name}.bam'
	run:
		os.makedirs(os.path.dirname(output.output_bam), exist_ok=True)
		# get passing read IDs
		qc = pandas.read_table(input.qc_file)
		qc = qc[ qc.passes_filtering ]
		passing_read_ids = set(qc[qc['passes_filtering']].read_id)
		# copy just those reads
		with pysam.AlignmentFile(input.input_bam) as bam_in:
			with pysam.AlignmentFile(output.output_bam, 'wb', template=bam_in) as bam_out:
				for r in bam_in.fetch():
					if r.query_name in passing_read_ids:
						bam_out.write(r)
		pysam.index(output.output_bam)

# Writes out reads, with sequence replaced with the reference sequence.
rule bam_matching_ref:
	input:
		'bam_1/{sample_name}.bam'
	output:
		'bam_matching_ref/{sample_name}.bam'
	run:
		seq.nanopore.polish.write_bam_matching_reference(
			AANCR_fasta,
			input[0],
			output[0])

# "Polishes" reads, and writes events, using f5c.
# (This writes various indexing files in fastq/, which seems like
# somewhat bad form, but a bit tricky to avoid.)
rule polish:
	input:
		fastq = DATA_DIR + 'fastq_files/{sample_name}.fq.gz',
		fast5_dir = DATA_DIR + 'fast5_files/{sample_name}/',
		bam = 'bam_matching_ref/{sample_name}.bam'
	output:
		events_file='events/{sample_name}.mergedEvent.tsv.gz'
	run:
		# pdb.set_trace()
		polisher = seq.nanopore.polish.Polisher(
			input.fast5_dir, input.fastq, input.bam,
			# we use the actual filename for the Snakemake
			# rule (so that Snakemake can track whether the
			# file was created), but the Polisher expects
			# the "base name"
			eventOutputBase = re.sub(
				'.mergedEvent.tsv.gz$',
				'',
				output.events_file))
		print('indexing...')
		polisher.indexReads()
		print('writing events...')
		polisher.write_merged_events(AANCR_fasta)

# Writes out per-base RDD counts.
rule RDD:
	input:
		'bam_1/{sample_name}.bam'
	output:
		'RDD_3/{sample_name}.csv.gz'
	shell:
		'scripts/base_count.py {input} {output} {AANCR_fasta} {regions_file}'


rule error_stats:
	input:
                ['RDD_3/' + s + '.csv.gz'
			for s in sample_base]
	output:
		'scripts/error_rates_2.xlsx'
	shell:
		'scripts/error_rates_2.py'


rule dwell_time_plots:
	input:
                ['events/' + s + '.mergedEvent.tsv.gz'
			for s in sample_base]
	output:
		'output/Dwell time for sugar modifications, 100%.pdf'
	shell:
		'scripts/event_dist_at_site.py'

# Writes out a matrix of samples for each sample.
rule squiggle_matrix:
	input:
		fastq='fastq/{sample_name}.fastq.gz',
		events_file='events/{sample_name}.mergedEvent.tsv.gz',
		bam_file='bam_no_splicing/{sample_name}.bam'
	output:
		squiggle_matrix_file=(
			'squiggle_matrix/{region}/{sample_name}.npz')
	run:
		read_data = seq.nanopore.squiggle.read_event.ReadEventData(
			input.fastq, input.events_file)
    		traces = read_data.get_raw_traces_at_region(region,
			max_reads=1000)
		# get called bases as a matrix
		bam = pysam.AlignmentFile(input.bam_file)
		called_bases = seq.nanopore.squiggle.matrix.get_bases_as_matrix(
			bam, traces, region)
		# get matrix of squiggle data
		s = seq.nanopore.squiggle.matrix.get_samples(
			region, '+', traces)
		np.savez_compressed(output.squiggle_matrix_file,
			base = s['base'],
			time = s['time'],
			dwell = s['dwell'],
			current = s['current'],
			called_bases = called_bases)

# Writes out labelled squiggles in compressed Numpy .npz format.
rule labelled_squiggle:
	input:
		fastq='fastq/{sample_name}.fastq.gz',
		events_file='events/{sample_name}.mergedEvent.tsv.gz'
	output:
		labelled_squiggle_file=(
			'labelled_squiggle/{region1}/{sample_name}.npz')
	run:
		read_data = seq.nanopore.squiggle.read_event.ReadEventData(
			input.fastq, input.events_file)
		seq.nanopore.squiggle.matrix.write_labelled_squiggles_as_numpy(
			read_data, output.labelled_squiggle_file, region1,
			max_reads=1000)

# Summary stats about events.
rule event_stats:
	input:
		fastq = DATA_DIR + 'fastq_files/{sample_name}.fq.gz',
		events_file='events/{sample_name}.mergedEvent.tsv.gz'
	output:
		stats_output=('event_stats/{region}/{sample_name}.csv.gz')
	run:
		read_data = seq.nanopore.squiggle.read_event.ReadEventData(
			input.fastq, input.events_file)
		events = read_data.get_events_at_region(region,
			include_dwell_time=True)
		event_stats = seq.nanopore.squiggle.stat.event_stats.event_stats_by_position(
			events)
		event_stats.to_csv(output.stats_output)

# T-tests between event statistics.
rule event_t_test:
        input:
                fastq = DATA_DIR + 'fastq_files/{sample_name}.fq.gz',
                events_file='events/{sample_name}.mergedEvent.tsv.gz'
        output:
                t_test_output=('event_t_test/{region}/{sample_name}.csv.gz')
	run:
		# get name of corresponding control sample
		modified_short_name = modified_sample_table.loc[
			wildcards.sample_name].short_name
		control_sample_name = control_sample_table.loc[
			modified_short_name].FASTQ_file_base_name
		# get modified events
                mod_events = seq.nanopore.squiggle.read_event.ReadEventData(
                	input.fastq, input.events_file
		).get_events_at_region(region)
		# get control events
                control_events = seq.nanopore.squiggle.read_event.ReadEventData(
                	f'fastq/{control_sample_name}.fastq.gz',
                	f'events/{control_sample_name}.mergedEvent.tsv.gz'
		).get_events_at_region(region)
		# do t-test, and write out results
		event_t_test = seq.nanopore.squiggle.stat.event_stats.t_test(
			mod_events, control_events)
		event_t_test.to_csv(output.t_test_output)

# Summary stats about events.
rule event_summary_stats:
	input:  
                fastq = DATA_DIR + 'fastq_files/{sample_name}.fq.gz',
		events_file='events/{sample_name}.mergedEvent.tsv.gz'
	output: 
		stats_output=('event_summary_stats/{sample_name}.csv.gz')
	run:
		# ??? don't know why this needs to be read in again
		sample_table = pandas.read_csv('IVT_samples.csv')
		# get short name of modification
		sample_table = sample_table.set_index(['FASTQ_file_base_name'])
		short_name = sample_table.loc[
			wildcards.sample_name].short_name
		# get "base which was modified"
		short_name_to_modified_base = {
			'Am': 'A', 'Cm': 'C', 'Gm': 'G', 'Um': 'U',
			'm1A': 'A', 'm6A': 'A', 'biotin-C': 'C',
			'hm5C': 'C', 'm5C': 'C', 'Y': 'U',
			'Cm + 5mM MnCl': 'C'}
		modified_base = short_name_to_modified_base[short_name]
		# XXX hack to deal with 'U' being written as 'T' in the output file
		if modified_base == 'U':
			modified_base = 'T'
		# get data
		if short_name in ['Am', 'Cm', 'Gm', 'Um']:
			region = 'chr19:45406985-45407485'
		else:   
			region = 'chr19:45406985-45408892'
		# XXX for practice
		# region = 'chr19:45406985-45407015'
		# just using the entire region, for all samples (even though some of
		# them aren't present in the entire region)
		region = 'chr19:45406985-45408892'
		read_data = seq.nanopore.squiggle.read_event.ReadEventData(
			input.fastq, input.events_file)
		events = read_data.get_events_at_region(region, include_dwell_time=True)
		events = events[ events.central_base==modified_base ]
		event_stats = seq.nanopore.squiggle.stat.event_stats.event_stats_summary(
			events)
		event_stats.to_csv(output.stats_output)

